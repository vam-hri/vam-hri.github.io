<!DOCTYPE html>

<html lang="en" class="gr__getbootstrap_com">

<head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<link rel="icon" href="favicon.ico">

<title>VAM-HRI 2018</title>

<!-- Bootstrap core CSS -->
<link href="./bootstrap-4.0.0/css/bootstrap.min.css" rel="stylesheet">

<!-- Custom styles for this template -->
<link href="./files/css/cover.css" rel="stylesheet">

<!-- jQuery -->
<script src="files/js/jquery-3.2.1.min.js"></script>

<!-- Bootstrap core JavaScript
================================================== -->
<script src="./files/js/popper.min.js"></script>
<script src="./bootstrap-4.0.0/js/bootstrap.min.js"></script>
<!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
<script src="./files/js/ie10-viewport-bug-workaround.js"></script>

<script type="text/javascript">

$(function() {

$('.nav-link').click( function(){
$('html,body').animate({scrollTop: $($(this).attr('href')).offset().top}, 'slow');
});

});

</script>

</head>

<body data-gr-c-s-loaded="true" style="position: relative;" data-spy="scroll" data-target=".inner" data-offset="100" id="body">

<div class="site-wrapper">

<div class="site-wrapper-inner">

<div class="cover-container">

<div class="masthead clearfix" style="z-index:999;">
<div class="inner">
<!-- <h3 class="masthead-brand">VAM4HRI 2018</h3> -->
<nav class="nav nav-masthead">
<a class="nav-link active" href="#body">Home</a>
<a class="nav-link" href="#about">About</a>
<a class="nav-link" href="#proceedings">Proceedings</a>
<a class="nav-link" href="#team">Team</a>
</nav>
</div>
</div>

<div class="inner cover">
<!-- <h1 class="cover-heading">VAM 2018</h1> -->
<img src="files/images/logo.png" width="100%">
<p class="lead" style="margin-top: 20px; color: #ff6161;">
<strong>
March 5th, 2018.
</strong>
</p>
<p class="lead" style="margin-top: 10px; color: #ffd761;">
<strong>
The Inaugural International Workshop on Virtual, Augmented and Mixed Reality for Human-Robot Interaction.
</strong>
</p>
<p class="lead">
<a href="./files/pdf/vam-hri-2018-proceedings.pdf" class="btn btn-lg btn-dark nav-link">Proceedings</a>
</p>
</div>

<div class="mastfoot">
<div class="inner">
<p>Collocated with <a href="http://humanrobotinteraction.org/2018/" target="_blank">HRI 2018</a></p>
</div>
</div>

</div>

</div>

</div>

<div class="jumbotron" id="about" style="background-color: #000;">

<div class="row justify-content-center">

<div class="row">

<div class="col-xs-12 col-sm-12 col-md-8 col-lg-9 col-xl-9" style="padding-left: 20px;padding-right: 20px;">

<img src="files/images/logo_bw.png" width="100%" style="margin-bottom:40px;" class="d-none d-md-block">

<p>
The 1st International Workshop on Virtual, Augmented, and Mixed Reality for Human-Robot Interactions (VAM-HRI) seeks to bring together researchers from different areas of the HRI, Robotics, Artificial Intelligence, and Mixed Reality communities with the goal of identifying and codifying the challenges in the emerging area of mixed reality interactions between humans and robots.  While there has been sporadic work in this area during the last decade, with interest mostly confined to industrial applications, recent advances in the space of mixed reality technologies has opened up exciting avenues of research in the field of HRI.  This is thus the first workshop of its kind at an academic AI or Robotics conference, and is intended to serve as a timely call to arms to the academic community in response to the growing promise of this emerging field.
</p>

<table class="table" style="margin-bottom: 10px;">
<tbody class="text-center">
<tr><th class="text-center">The workshop covers contributions across a wide range of topics in Augmented and Virtual Reality for HRI, including but not limited to:</th></tr>
<tr><td>AR-based intention communication</td></tr>
<tr><td>AR-based behavior explanation</td></tr>
<tr><td>AR/VR for robot testing and diagnostics</td></tr>
<tr><td>VR for HRI human-subject experimentation</td></tr>
<tr><td>Efficient representations for AR/VR</td></tr>
<tr><td>Mixed-reality language grounding</td></tr>
<tr><td>AR-augmented natural language generation</td></tr>
<tr><td>AR-enabled robot control interfaces</td></tr>
<tr><td>Hardware/software architectures for AR/VR-based HRI</td></tr>
<tr><td>HRI problems that can benefit most from emerging AR and VR technologies</td></tr>
<tr><td></td></tr>
</tbody>
</table>

</div>              

<div class="d-none d-md-block col-md-4 col-lg-3 col-xl-3">
<a class="twitter-timeline" data-height="1000" data-partner="tweetdeck" data-theme="dark" href="https://twitter.com/tchakra2/timelines/964752265880862721?ref_src=twsrc%5Etfw">VAM-HRI 2018 - Curated tweets by tchakra2</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

</div>
</div>

</div>

<div id="proceedings" class="row">
<div class="col-xs-12 col-lg-12" style="background-color:rgba(0, 0, 0, 0.1);padding: 20px;">
<a href="./files/pdf/vam-hri-2018-proceedings.pdf">
<h1>Download Proceedings <span class="badge badge-pill badge-success">** New **</span></h1>
</a>
</div>
</div>


<div id="schedule" class="" style="background-color:#fbe5d6;">

<div class="row">
<div class="col-xs-12 col-lg-12" style="background-color:rgba(0, 0, 0, 0.1);">
<h1 style="margin:20px;color:black;text-transform:capitalize;">Schedule (March 5th 2018; Room: Grant Park D)</h1>
</div>
</div>

<div class="row">
<div class="col-xs-12 col-lg-12" style="background-color:rgba(231, 249, 256, 1);">
<h4 style="margin:20px;color:black;text-transform:capitalize;">2:00-2:05pm - Introduction</h4>
</div>
</div>

<div class="row">
<div class="col-xs-12 col-lg-12" style="background-color:rgba(0, 0, 0, 0.1);">
<h2 style="margin:20px;color:black;text-transform:capitalize;">2:05-2:45pm - Invited Talk</h2>
</div>
</div>

<div class="row">

<div class="col-xs-12 col-md-4 col-lg-4 team" style="background-color: rgba(225, 255, 255, 0.77);">
<div class="text-center">
<img class="rounded-circle" src="./files/images/blair.png">
</div>
<br>
<div class="row">
<p style="margin: 0px; color:black; font-size: smaller;">
Blair MacIntyre is a Principle Research Scientist in the Emerging Technologies group at Mozilla, and a Professor of Interactive Computing at Georgia Tech. He has been doing AR research and development since 1991, founded the Augmented Environments Lab at Georgia Tech in 1999, and joined Mozilla in 2016. He has been working on bringing AR to the web since 2008, when he started the open-source Argon project at Georgia Tech. At Mozilla, he is leading the effort to bring high performance AR to commodity web browsers. He has worked on AR systems in military, industrial, educational, entertainment, and gaming domains, and consults on technical and legal issues in AR.
</p>
<br>
</div>
<div class="text-center">
<a class="btn btn-outline-dark" href="http://blairmacintyre.me/" target="_blank" role="button">Homepage &raquo;</a>
</div>      
</div>

<div class="col-xs-12 col-md-8 col-lg-8 team" style="background-color: rgba(155, 255, 255, 0.37);">
<div class="row" style="padding: 10px; padding-left: 15px; background-color: rgba(0,0,0,0.1);">
<h5 style="color: black; margin-bottom: 0px;">Talk Title</h5>
</div>
<div class="row" style="padding: 15px;">
<h3 style="color: black;">Making the Hidden Visible</h3>
</div>
<div class="row" style="padding: 10px; padding-left: 15px; background-color: rgba(0,0,0,0.1);">
<h5 style="color: black; margin-bottom: 0px;">Abstract</h5>
</div>
<div class="row" style="padding: 15px;">
<p style="margin: 0px; color:black; text-align: left;">
Augmented Reality is a technology that overlays digital information on a personâ€™s view of the world around them, to enhance their perception or understanding. Systems that use AR techniques rely on rich knowledge  of the world near the viewer; when people are working with robots and other sensor-based systems, AR techniques can be used to leverage and expose otherwise-hidden digital information those systems have. In our work, we have explored how to use AR to communicate sensor information to workers on assembly lines, and the state and plans of multi-robot systems in the Robotarium.  We are particularly interesting in leveraging web-based technologies to create AR interfaces, simplifying creation and distribution of platform-independent network-based systems.  In this talk I will discuss our past work, and the opportunities WebXR may bring for HRI.
</p>
</div>
</div>


</div>

<div class="row">
<div class="col-xs-12 col-lg-12" style="background-color:rgba(0, 0, 0, 0.1);">
<h2 style="margin:20px;color:black;text-transform:capitalize;">2:45-3:20pm - Lightning Talks - 1</h2>
</div>
</div>

<ul class="list-group" style="color: black; text-align: left;">
<li class="list-group-item">Nikolaos Katzakis and Frank Steinicke.<br><strong>Excuse me! Perception of Abrupt Direction Changes Using Body Cues and Paths on Mixed Reality Avatars</strong></li>
<li class="list-group-item">Sebastian Meyer Zu Borgsen, Patrick Renner, Florian Lier, Sven Wachsmuth and Thies Pfeiffer.<br><strong>Improving Human-Robot Handover Research by Mixed-Reality Techniques</strong></li>
<li class="list-group-item">Jordan Allspaw, Jonathan Roche, Adam Norton and Holly Yanco.<br><strong>Teleoperating a Humanoid Robot with Virtual Reality</strong></li>
<a href="files/pdf/hri2018_edited.pdf" target="_blank" style="color: black;"><li class="list-group-item">Yeonju Oh, Ramviyas Parasuraman, Tim McGraw and Byung-Cheol Min.<br><strong>360 VR Based Robot Teleoperation Interface for Virtual Tour</strong></li></a>
<a href="files/pdf/16.pdf" target="_blank" style="color: black;"><li class="list-group-item">Tom Williams.<br><strong>A Framework for Robot-Generated Mixed-Reality Deixis</strong></li></a>
<li class="list-group-item">Michael Iuzzolino, Michael Walker and Daniel Szafir.<br><strong>Virtual-to-Real-World Transfer Learning for Robot Navigation</strong></li>
</ul>


<div class="row">
<div class="col-xs-12 col-lg-12" style="background-color:rgba(0, 0, 0, 0.1);">
<h2 style="margin:20px;color:black;text-transform:capitalize;">3:20-4:00pm - Poster Session - 1 (+ Coffee Break)</h2>
</div>
</div>

<ul class="list-group" style="color: black; text-align: left;">
<li class="list-group-item">Nikolaos Katzakis and Frank Steinicke.<br><strong>Excuse me! Perception of Abrupt Direction Changes Using Body Cues and Paths on Mixed Reality Avatars</strong></li>
<li class="list-group-item">Daniele Sportillo, Alexis Paljic, Luciano Ojeda, Giacomo Partipilo, Philippe Fuchs and Vincent Roussarie.<br><strong>Training semi-autonomous vehicle drivers with Extended Reality</strong></li>
<a href="files/pdf/Paper3_Arevalo.pdf" target="_blank" style="color: black;"><li class="list-group-item">Stephanie Arevalo Arboleda, Max Pascher and Jens Gerken.<br><strong>Opportunities and Challenges in Mixed-Reality for an inclusive Human-Robot Collaboration Environment</strong></li></a>
<li class="list-group-item">Jingxin Zhang, Eike Langbehn, Dennis Krupke and Frank Steinicke.<br><strong>A 360Â° Video-based Robot Platform for Telepresent Redirected Walking</strong></li>
<li class="list-group-item">David Goedicke, Jamy Li, Vanessa Evers and Wendy Ju.<br><strong>VR-OOM: Virtual Reality On-rOad driving siMulation</strong></li>
<a href="files/pdf/VAM_HRI_Workshop_Marten_Lager.pdf" target="_blank" style="color: black;"><li class="list-group-item">MÃ¥rten Lager, Elin Anna Topp and Jacek Malec.<br><strong>Remote Operation of Unmanned Surface Vessel through Virtual Reality - a low cognitive load approach</strong></li></a>
<li class="list-group-item">Sebastian Meyer Zu Borgsen, Patrick Renner, Florian Lier, Sven Wachsmuth and Thies Pfeiffer.<br><strong>Improving Human-Robot Handover Research by Mixed-Reality Techniques</strong></li>
<a href="files/pdf/augmented-reality-interfaces%20(2).pdf" target="_blank" style="color: black;"><li class="list-group-item">Shelly Bagchi and Jeremy Marvel.<br><strong>Towards Augmented Reality Interfaces for Human-Robot Interaction in Manufacturing Environments</strong></li></a>
<a href="files/pdf/ract_VAM_HRI_2018.pdf" target="_blank" style="color: black;"><li class="list-group-item">LÃ¦rke Isabella NÃ¸rregaard Hansen, Niklas Vinther, Lukas Stranovsky, Mark Philip Philipsen, Haiyan Wu and Thomas Baltzer Moeslund.<br><strong>Collaborative Meat Processing in Virtual Reality - How should a robot approach a human coworker?</strong></li></a>
<li class="list-group-item">Jordan Allspaw, Jonathan Roche, Adam Norton and Holly Yanco.<br><strong>Teleoperating a Humanoid Robot with Virtual Reality</strong></li>
<a href="files/pdf/hri2018_edited.pdf" target="_blank" style="color: black;"><li class="list-group-item">Yeonju Oh, Ramviyas Parasuraman, Tim McGraw and Byung-Cheol Min.<br><strong>360 VR Based Robot Teleoperation Interface for Virtual Tour</strong></li></a>
<a href="files/pdf/HRI2018-VR-Swarm-Haring.pdf" target="_blank" style="color: black;"><li class="list-group-item">Kerstin Sophie Haring, Dylan Muramoto, Natan L Tenhundfeld, Mormon Redd and Brian Tidball.<br><strong>Analysis of Using Virtual Reality (VR) for Command and Control Applications of Multi-Robot Systems</strong></li></a>
<li class="list-group-item">Nhan Tran, Joshua Rands and Tom Williams.<br><strong>A Hands-Free Virtual-Reality Teleoperation Interface for Wizard-of-Oz Control</strong></li>
<a href="files/pdf/16.pdf" target="_blank" style="color: black;"><li class="list-group-item">Tom Williams.<br><strong>A Framework for Robot-Generated Mixed-Reality Deixis</strong></li></a>
<li class="list-group-item">Michael Iuzzolino, Michael Walker and Daniel Szafir.<br><strong>Virtual-to-Real-World Transfer Learning for Robot Navigation</strong></li>
</ul>

<div class="row">
<div class="col-xs-12 col-lg-12" style="background-color:rgba(0, 0, 0, 0.1);">
<h2 style="margin:20px;color:black;text-transform:capitalize;">4:00-4:30pm - Lightning Talks - 2</h2>
</div>
</div>

<ul class="list-group" style="color: black; text-align: left;">
<li class="list-group-item">Francesca Stramandinoli, Kin Gwn Lore, Jeffrey R. Peters, Paul C. Oâ€™neill, Binu M. Nair, Richa Varma, Julian C. Ryde, Jay T. Miller and Kishore K. Reddy.<br><strong>Robot Learning from Human Demonstration in Virtual Reality</strong></li>
<a href="files/pdf/learning-crowdsourced-virtual%20(1).pdf" target="_blank" style="color: black;"><li class="list-group-item">David Whitney, Eric Rosen and Stefanie Tellex.<br><strong>Learning from Crowdsourced Virtual Reality Demonstrations</strong></li></a>
<a href="files/pdf/novitzky-vam-workshop-sigconf.pdf" target="_blank" style="color: black;"><li class="list-group-item">Michael Novitzky, Michael R. Benjamin, Paul Robinette, Hugh R.R. Dougherty, Caileigh Fitzgerald and Henrik Schmidt.<br><strong>Virtual Reality for Immersive Simulated Experiments of Human-Robot Interactions in the Marine Environment</strong></li></a>
<a href="files/pdf/projection-aware.pdf" target="_blank" style="color: black;"><li class="list-group-item">Tathagata Chakraborti, Sarath Sreedharan, Anagha Kulkarni and Subbarao Kambhampati.<br><strong>A Mixed Reality Workspace for Proximal Human-in-the-Loop Operation of Robots</strong></li></a>
<li class="list-group-item">Heni Ben Amor, Ramsundar Kalpagam Ganesan, Yash Rathore and Heather Ross.<br><strong>Intention Projection for Human-Robot Collaboration with Mixed Reality Cues</strong></li>
</ul>

<div class="row">
<div class="col-xs-12 col-lg-12" style="background-color:rgba(231, 249, 256, 1);">
<h6 style="margin:20px;color:black;text-transform:capitalize;">4:30-4:35pm - Poster Setup</h6>
</div>
</div>

<div class="row">
<div class="col-xs-12 col-lg-12" style="background-color:rgba(0, 0, 0, 0.1);">
<h2 style="margin:20px;color:black;text-transform:capitalize;">4:35-5:15pm - Poster Session - 2</h2>
</div>
</div>

<ul class="list-group" style="color: black; text-align: left;">
<li class="list-group-item">Maxwell Bennett, Tom Williams, Daria Thames and Matthias Scheutz.<br><strong>Investigating Interactions with Teleoperated and Autonomous Humanoids Using a Suit-Based VR Teleoperation Interface</strong></li>
<li class="list-group-item">Francesca Stramandinoli, Kin Gwn Lore, Jeffrey R. Peters, Paul C. Oâ€™neill, Binu M. Nair, Richa Varma, Julian C. Ryde, Jay T. Miller and Kishore K. Reddy.<br><strong>Robot Learning from Human Demonstration in Virtual Reality</strong></li>
<a href="files/pdf/learning-crowdsourced-virtual%20(1).pdf" target="_blank" style="color: black;"><li class="list-group-item">David Whitney, Eric Rosen and Stefanie Tellex.<br><strong>Learning from Crowdsourced Virtual Reality Demonstrations</strong></li></a>
<a href="files/pdf/Submission_17_embedding_ar_in_industrial_hri_applications.pdf" target="_blank" style="color: black;"><li class="list-group-item">Manfred SchÃ¶nheits and Florian Krebs.<br><strong>Embedding AR In Industrial HRI Applications</strong></li></a>
<li class="list-group-item">Jeffrey Too Chuan Tan, Yoshiaki Mizuchi, Yoshinobu Hagiwara and Tetsunari Inamura.<br><strong>Representation of Embodied Collaborative Behaviors in Cyber-Physical Human-Robot Interaction with Immersive User Interfaces</strong></li>
<a href="files/pdf/study-robot-manipulation.pdf" target="_blank" style="color: black;"><li class="list-group-item">Eric Rosen, David Whitney, Elizabeth Phillips, Daniel Ullman and Stefanie Tellex.<br><strong>Testing Robot Teleoperation using a Virtual Reality Interface with ROS Reality</strong></li></a>
<li class="list-group-item">Camilo Perez Quintero, Sarah Li, Cole Shing, Wesley Chan, Sara Sara Sheikholeslami, Machiel Van der Loos and Elizabeth Croft.<br><strong>Robot Programming Through Augmented Trajectories</strong></li>
<a href="files/pdf/novitzky-vam-workshop-sigconf.pdf" target="_blank" style="color: black;"><li class="list-group-item">Michael Novitzky, Michael R. Benjamin, Paul Robinette, Hugh R.R. Dougherty, Caileigh Fitzgerald and Henrik Schmidt.<br><strong>Virtual Reality for Immersive Simulated Experiments of Human-Robot Interactions in the Marine Environment</strong></li></a>
<a href="files/pdf/vam-hri_paper_23.pdf" target="_blank" style="color: black;"><li class="list-group-item">Sanket Gaurav, Zainab Al-Qurashi, Amey Barapatre and Biran D. Ziebart.<br><strong>Enabling Effective Robotic Teleoperation using Virtual Reality and Correspondence Learning via Neural Network</strong></li></a>
<a href="files/pdf/Cheli_VAMHRI_2018.pdf" target="_blank" style="color: black;"><li class="list-group-item">Mark Cheli, Jivko Sinapov, Ethan Danahy and Chris Rogers.<br><strong>Towards an Augmented Reality Framework for K-12 Robotics Education</strong></li></a>
<li class="list-group-item">Christopher Peters, Fangkai Yang, Himangshu Saikia, Chengjie Li and Gabriel Skantze.<br><strong>Towards the use of Mixed Reality for HRI Design via Virtual Robots</strong></li>
<a href="files/pdf/projection-aware.pdf" target="_blank" style="color: black;"><li class="list-group-item">Tathagata Chakraborti, Sarath Sreedharan, Anagha Kulkarni and Subbarao Kambhampati.<br><strong>A Mixed Reality Workspace for Proximal Human-in-the-Loop Operation of Robots</strong></li></a>
<li class="list-group-item">Tathagata Chakraborti, Andrew Dudley and Subbarao Kambhampati.<br><strong>v2v Communication for Augmenting Reality Enabled Smart HUDs to Increase Situational Awareness of Drivers</strong></li>
<a href="files/pdf/Implementation%20of%20AR%20in%20Autonomous%20Warehouses%20-%20Challenges%20and%20Opportunities_reviewed.pdf" target="_blank" style="color: black;"><li class="list-group-item" style="color: silver;">[Unable to attend] David Puljiz, Gleb Gorbachev and Bjoern Hein.<br><strong>Implementation of Augmented Reality in Autonomous Warehouses: Challenges and Opportunities</strong></li></a>
<li class="list-group-item">Heni Ben Amor, Ramsundar Kalpagam Ganesan, Yash Rathore and Heather Ross.<br><strong>Intention Projection for Human-Robot Collaboration with Mixed Reality Cues</strong></li>
</ul>

<div class="row">
<div class="col-xs-12 col-lg-12" style="color:black;background-color:rgba(0, 0, 0, 0.1);">
<h2 style="margin:20px;text-transform:capitalize;">5:15-5:55pm Panel</h2>
<div style="text-align:left;">
<li class="list-group-item">Panel Topic - <strong>Virtual, Augmented and Mixed Reality in Robotics: Progress, Opportunities, Challenges</strong></li>
<li class="list-group-item">[Panelist] <strong>Blair MacIntyre</strong><br>Professor, Georgia Tech &amp; Principle Research Scientist, Emerging Technologies Group, Mozilla</li>
<li class="list-group-item">[Panelist] <strong>Michael "Misha" Novitzky</strong><br>Post Doctoral Associate, Laboratory for Marine Sensing Systems (LAMSS), MIT</li>
<li class="list-group-item">[Panelist] <strong>Cindy Bethel</strong><br>Associate Professor of Computer Science and Engineering, Mississippi State University</li>
<li class="list-group-item">[Host] <strong>Heni Ben Amor</strong><br>Professor, Arizona State University</li>
</div>
</div>
</div>

<div class="row">
<div class="col-xs-12 col-lg-12" style="background-color:rgba(255, 255, 255, 1);">
<h6 style="margin:20px;color:black;text-transform:capitalize;">5:55-6:00pm - Wrap Up</h6>
</div>
</div>

</div>

<img src="files/images/graphic.png" width="100%" height="auto" class="d-none d-md-block">


<div id="team" class="" style="background-color:#fbe5d6; margin-bottom: 20px;">

<div class="row">
<div class="col-xs-12 col-lg-12" style="background-color:rgba(0, 0, 0, 0.1);">
<h2 style="margin:20px;color:black;text-transform:capitalize;">Organizing Committee</h2>
</div>
</div>

<div class="row">

<div class="col-xs-12 col-md-3 col-lg-3 team" style="background-color: rgba(9, 89, 172, 0.37);">
<div class="text-center">
<img class="rounded-circle" src="./files/images/tom.jpg">
</div>
<br>
<div class="row">
<p style="margin: 0px; color:black;">
Tom Williams is an assistant professor of computer science at Colorado School of Mines, where he directs the Mines Interactive Robotics Research (MIRROR) Lab. His research focuses on enabling and understanding natural language based human-robot interaction, especially as applied to assistive and search-and-rescue robotics, and has been featured in international AI and Robotics conferences such as HRI, AAAI, AAMAS, IROS, RSS, and INLG, as well as the JHRI and RAS journals. Tom served as program committee co-chair for the 2016 HRI Pioneers workshop and AAAI Fall Symposium on AI-for-HRI (AI-HRI), is a special track session chair for EAAI 2018, and is a Senior Program Committee member for AAAI 2018.
</p>
<br>
</div>
<div class="text-center">
<a class="btn btn-outline-dark" href="http://mirrorlab.mines.edu" target="_blank" role="button">Homepage &raquo;</a>
</div>      
</div>

<div class="col-xs-12 col-md-3 col-lg-3 team" style="background-color: rgba(205, 173, 161, 0.91);">
<div class="text-center">
<img class="rounded-circle" src="./files/images/dan.jpg">
</div>
<br>
<div class="row">
<p style="margin: 0px; color:black;">
Daniel Szafir is an assistant professor in the Department of Computer Science and ATLAS Institute at the University of Colorado Boulder, where he directs the Interactive Robotics and Novel Technologies (IRON) Lab. His research at the intersection of robotics and human-computer interaction (HCI) focuses on investigating how novel technologies can mediate interactions between people and autonomous systems. His work has been featured in international robotics, virtual/augmented reality, and design conferences including HRI, ISMAR, 3DUI (now merged with IEEE VR), CHI, and DIS as well the International Journal of Robotics Research (IJRR). Daniel has previously served as the Videos and Demos Co-Chair for HRI 2017, an organizing committe member for the RSS 2017 Workshop on Bridging the Gap in Space Robotics, the panel chair for the 2015 HRI Pioneers workshop, and a program committee member for HRI, RO-MAN, CHI, RSS, and ARSO.
</p>
<br>
</div>
<div class="text-center">
<a class="btn btn-outline-dark" href="http://atlas.colorado.edu/iron/" target="_blank" role="button">Homepage &raquo;</a>
</div>      
</div>

<div class="col-xs-12 col-md-3 col-lg-3 team" style="background-color: rgba(152, 156, 197, 0.46);">
<div class="text-center">
<img class="rounded-circle" src="./files/images/tathagata.jpg">
</div>
<br>
<div class="row">
<p style="margin: 0px; color:black;">
Tathagata Chakraborti is a senior Ph.D. student at Arizona State University working in the Yochan Lab with Prof. Subbarao Kambhampati. His research interests include planning with humans in the loop, with applications in task planning for human-robot teaming and cohabitation, and proactive decision support. His research has featured in premier research conferences and workshops in the field of artificial intelligence and robotics worldwide, including AAAI, IJCAI, AAMAS, ICAPS, IROS, ICRA, etc. He has also received the back to back IBM Ph.D. Fellowship and multiple University Graduate Fellowship Awards in recognition of his work. He has been on the organizing team for the Workshop on Multi-agent Interaction without Prior Coordination (MIPC) at AAMAS'17 and Workshop on Explainable AI (XAI) at IJCAI 2017, as well as on the Review Process Committee of IJCAI 2016 and the Program Committee of IJCAI 2018. Recently, he was the team lead of &AElig;Robotics which featured in the US Finals of the Microsoft Imagine Cup 2017 with an innovative solution for intention projection between humans and robots using augmented reality.
</p>
<br>
</div>
<div class="text-center">
<a class="btn btn-outline-dark" href="http://www.public.asu.edu/~tchakra2/" target="_blank" role="button">Homepage &raquo;</a>
</div>      
</div>

<div class="col-xs-12 col-md-3 col-lg-3 team" style="background-color: rgba(152, 156, 197, 0.0);">
<div class="text-center">
<img class="rounded-circle" src="./files/images/heni.jpg">
</div>
<br>
<div class="row">
<p style="margin: 0px; color:black;">
Heni Ben Amor is an Assistant Professor at Arizona State University where he heads the ASU Interactive Robotics Lab. Prior to that, he was a Research Scientist at the Institute for Robotics and Intelligent Machines at GeorgiaTech in Atlanta. Heni studied Computer Science at the University of Koblenz-Landau (GER) and earned a Ph.D in robotics from the Technical University Freiberg and the University of Osaka in 2010. Before moving to the US, Heni was a postdoctoral scholar at the Technical University Darmstadt. Heni's research topics focus on artificial intelligence, machine learning, human-robot interaction, robot vision, and automatic motor skill acquisition. He received the highly competitive Daimler-and-Benz Fellowship, as well as several best paper awards at major robotics and AI conferences. He is in the program committee of various AI and robotics conferences such as AAAI, IJCAI, IROS, and ICRA. 
</p>
<br>
</div>
<div class="text-center">
<a class="btn btn-outline-dark" href="http://interactive-robotics.engineering.asu.edu/" target="_blank" role="button">Homepage &raquo;</a>
</div>      
</div>
</div>

<div class="row">
<div class="col-xs-12 col-lg-12" style="background-color:rgba(0, 0, 0, 0.1);">
<h2 style="margin:20px;color:black;text-transform:capitalize;">Advisory Board</h2>
</div>
</div>

<div class="row" style="background-color: rgba(9, 89, 172, 0.37);margin-left:0px;margin-right:0px;">
<div class="col-12">
<p style="margin:15px;font-size:12px;text-align:left;color:black;">
<a target="_blank" href="https://hrilab.tufts.edu/people/matthias.php" style="color:black;"><strong>Matthias Scheutz</strong></a> is a professor of cognitive and computer science, and director of the <a target="_blank" href="https://hrilab.tufts.edu/" style="color:black;"><strong>Human-Robot Interaction Laboratory</strong></a> at Tufts University. His research interests include artificial intelligence, cognitive modeling, foundations of cognitive science, human-robot interaction, multi-scale agent-based models and natural language processing.
</p>
</div>
</div>

<div class="row" style="background-color:rgba(205, 173, 161, 0.91);margin-left:0px;margin-right:0px;">
<div class="col-12">
<p style="margin:15px;font-size:12px;text-align:left;color:black;">
<a target="_blank" href="http://rakaposhi.eas.asu.edu/" style="color:black;"><strong>Subbarao Kambhampati</strong></a> is a professor of Computer Science &amp; Engineering at Arizona State University, where he directs the <a target="_blank" href="http://rakaposhi.eas.asu.edu/yochan/" style="color:black;"><strong>Yochan research group</strong></a>. His research interests are primarily in Artificial Intelligence, and include planning and decision making, human-robot teaming and human-aware AI. He is an elected Fellow and the current president of the Association for the Advancement of Artificial Intelligence (AAAI).
</p>
</div>
</div>

<div class="row" style="background-color:rgba(152, 156, 197, 0.46);margin-left:0px;margin-right:0px;">
<div class="col-12">
<p style="margin:15px;font-size:12px;text-align:left;color:black;">
<a target="_blank" href="http://pages.cs.wisc.edu/~bilge/" style="color:black;"><strong>Bilge Mutlu</strong></a>  is an Associate Professor of Computer Science, Psychology, and Industrial Engineering at the University of Wisconsinâ€“Madison. He directs the <a target="_blank" href="http://hci.cs.wisc.edu/" style="color:black;"><strong>Wisconsin HCI Laboratory</strong></a>. His research program draws on technical, behavioral, and design perspectives to build human-centered principles for the design of robotic technologies.
</p>
</div>
</div>

<div class="row" style="background-color:rgba(152, 156, 197, 0.0);margin-left:0px;margin-right:0px;">
<div class="col-12">
<p style="margin:15px;font-size:12px;text-align:left;color:black;">
<a target="_blank" href="http://inside.mines.edu/~whoff/" style="color:black;"><strong>William Hoff</strong></a> is an associate professor in the department of Computer Science at the Colorado School of Mines. He teaches courses in computer vision and mobile application development. His research interests include computer vision and pattern recognition, with applications in robotics, video aided navigation, activity recognition, and augmented reality.
</p>
</div>
</div>

</div>

<img src="files/images/footer.png" width="100%">

</body>

</html>